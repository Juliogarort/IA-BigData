{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac7ac70f",
   "metadata": {},
   "source": [
    "# UD3 ‚Äì Actividad 3.2: Control de calidad de ficheros CSV con firma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa52ae",
   "metadata": {},
   "source": [
    "**Nombre y apellidos del alumno:**  \n",
    "*(Completa este apartado antes de comenzar la actividad)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188bea7",
   "metadata": {},
   "source": [
    "## Introducci√≥n\n",
    "\n",
    "La empresa **RetailCorp** opera en varios pa√≠ses europeos y gestiona su inventario de productos de forma distribuida.\n",
    "\n",
    "Cada d√≠a, los distintos almacenes env√≠an un *snapshot* del estado de su inventario en formato CSV a un sistema centralizado de anal√≠tica y planificaci√≥n.\n",
    "\n",
    "Durante un incidente reciente, se detect√≥ que algunos ficheros conten√≠an inconsistencias en su contenido y errores de integridad de archivo (hash incorrecto).\n",
    "\n",
    "El √°rea de **Data Engineering** ha decidido implementar un **control de calidad de entrada en el pipeline**, de forma que solo los ficheros correctos puedan avanzar hacia las etapas posteriores.\n",
    "\n",
    "El objetivo de esta pr√°ctica es **dise√±ar e implementar un control de calidad de ficheros CSV con verificaci√≥n de firma (SHA256/MD5)** de forma reproducible y automatizable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c34867",
   "metadata": {},
   "source": [
    "## Dataset de trabajo\n",
    "\n",
    "Se proporciona un fichero comprimido `RetailCorp-dataset-sample.tar.gz` que contiene una muestra de ficheros CSV de inventario junto con sus firmas.\n",
    "\n",
    "### Contenido\n",
    "- CSV: snapshots de inventario con campos `product_id`, `warehouse_id`, `snapshot_date`, `stock_units`, `reserved_units`.\n",
    "- Firma: archivos `.sha256` o `.md5` correspondientes a cada CSV.\n",
    "\n",
    "El alumno deber√° extraer los ficheros y procesarlos siguiendo los pasos de la actividad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee987b3",
   "metadata": {},
   "source": [
    "# **Tareas a realizar**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c98025",
   "metadata": {},
   "source": [
    "### Tarea 1 ‚Äì Preparaci√≥n del entorno\n",
    "\n",
    "Crear los directorios de trabajo (`incoming_raw`, `input`, `rejected`, `archive`) y extraer los ficheros del dataset proporcionado.\n",
    "Se proporciona una funci√≥n de preparaci√≥n del entorno, no hay que modificar nada  \n",
    "\n",
    "Aqu√≠ solo hay que analizar el c√≥digo para familiarizarse con la estructura de directorios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prep-env-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def prepare_environment(base_dir=\"data_pipeline\",\n",
    "                        dataset=\"RetailCorp-dataset-sample.tar.gz\"):\n",
    "    \"\"\"\n",
    "    Prepara la estructura de directorios y extrae el dataset inicial.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(f\"{base_dir}/incoming_raw\", exist_ok=True)\n",
    "    os.makedirs(f\"{base_dir}/input\", exist_ok=True)\n",
    "    os.makedirs(f\"{base_dir}/rejected\", exist_ok=True)\n",
    "    os.makedirs(f\"{base_dir}/archive\", exist_ok=True)\n",
    "    os.makedirs(f\"{base_dir}/logs\", exist_ok=True)\n",
    "\n",
    "    subprocess.run(\n",
    "        [\"tar\", \"-xzf\", dataset, \"-C\", f\"{base_dir}/incoming_raw\"],\n",
    "        check=True\n",
    "    )\n",
    "\n",
    "\n",
    "def reset_data_pipeline(base_dir=\"data_pipeline\"):\n",
    "    \"\"\"\n",
    "    Elimina completamente el directorio de trabajo del pipeline\n",
    "    para permitir una ejecuci√≥n limpia del proceso.\n",
    "    \"\"\"\n",
    "    if os.path.exists(base_dir):\n",
    "        subprocess.run(\n",
    "            [\"rm\", \"-rf\", base_dir],\n",
    "            check=True\n",
    "        )\n",
    "\n",
    "reset_data_pipeline()\n",
    "prepare_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec70dd",
   "metadata": {},
   "source": [
    "### Tarea 2 ‚Äì Implementar la funcionalidad de gesti√≥n de la integridad en los ficheros de entrada\n",
    "\n",
    "Se recomienda ir implement√°ndolo en varias funciones que permitan dividir y estructurar el proceso.  \n",
    "Puede realizarse en varias celdas.  \n",
    "\n",
    "Registrar en un fichero de log la informaci√≥n de cada fichero procesado, indicando:\n",
    "- Nombre de fichero\n",
    "- Estado (`OK`, `REJECTED`, `ERROR`)\n",
    "- Mensaje asociado\n",
    "\n",
    "Mover los ficheros correctos al directorio `input`, los rechazados a `rejected` y archivar las firmas en `archive`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "list-files-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import binascii\n",
    "\n",
    "def calculate_hash(file_path, algorithm='sha256', chunk_size=4096):\n",
    "    if algorithm == 'sha256':\n",
    "        hash_func = hashlib.sha256()\n",
    "        with open(file_path, 'rb') as f:\n",
    "            for chunk in iter(lambda: f.read(chunk_size), b''):\n",
    "                hash_func.update(chunk)\n",
    "        return hash_func.hexdigest()\n",
    "\n",
    "    elif algorithm == 'md5':\n",
    "        hash_func = hashlib.md5()\n",
    "        with open(file_path, 'rb') as f:\n",
    "            for chunk in iter(lambda: f.read(chunk_size), b''):\n",
    "                hash_func.update(chunk)\n",
    "        return hash_func.hexdigest()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Algoritmo no soportado.\")\n",
    "\n",
    "# Funci√≥n copiada del tutorial, mediante la cual calculamos el hash del fichero y detectar cambios o fallos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "860d4ed0-9834-46c7-a08a-65a4ca48e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_signature(sig_path):\n",
    "\n",
    "    with open(sig_path, 'r') as f:\n",
    "        content = f.read().strip()\n",
    "    # El hash es la primera palabra de la l√≠nea\n",
    "    expected_hash = content.split()[0]\n",
    "    \n",
    "    if sig_path.endswith('.sha256'):\n",
    "        algorithm = 'sha256'\n",
    "    elif sig_path.endswith('.md5'):\n",
    "        algorithm = 'md5'\n",
    "    else:\n",
    "        raise ValueError(f\"Extensi√≥n de firma no reconocida: {sig_path}\")\n",
    "    \n",
    "    return expected_hash, algorithm\n",
    "\n",
    "# Funci√≥n para leer el fichero asociado al csv y extraer el hash para comprobarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b5b9a0c-08de-4286-a434-cc8be8ac95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "LOG_FILE = \"data_pipeline/logs/integrity_check.log\"\n",
    "\n",
    "def log_event(filename, status, message):\n",
    "\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    line = f\"{timestamp} | {filename} | {status} | {message}\\n\"\n",
    "    with open(LOG_FILE, 'a') as f:\n",
    "        f.write(line)\n",
    "    print(line.strip())\n",
    "\n",
    "# Funci√≥n para procesar el fichero y registrar el estado y usar un timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "log-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "\n",
    "def process_files(base_dir=\"data_pipeline\"):\n",
    "    incoming = f\"{base_dir}/incoming_raw\"\n",
    "    input_dir = f\"{base_dir}/input\"\n",
    "    rejected_dir = f\"{base_dir}/rejected\"\n",
    "    archive_dir = f\"{base_dir}/archive\"\n",
    "\n",
    "    # Buscamos todos los ficheros CSV en incoming_raw (sin duplicados)\n",
    "    csv_files = list(set(\n",
    "        glob.glob(f\"{incoming}/**/*.csv\", recursive=True) +\n",
    "        glob.glob(f\"{incoming}/*.csv\")\n",
    "    ))\n",
    "\n",
    "    if not csv_files:\n",
    "        print(\"No se encontraron ficheros CSV en incoming_raw.\")\n",
    "        return\n",
    "\n",
    "    for csv_path in csv_files:\n",
    "        filename = os.path.basename(csv_path)\n",
    "\n",
    "        sig_path = None\n",
    "        for ext in ['.sha256', '.md5']:\n",
    "            candidate = csv_path + ext\n",
    "            if os.path.exists(candidate):\n",
    "                sig_path = candidate\n",
    "                break\n",
    "\n",
    "        if sig_path is None:\n",
    "            shutil.move(csv_path, os.path.join(rejected_dir, filename))\n",
    "            log_event(filename, \"REJECTED\", \"No se encontr√≥ fichero de firma asociado\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            expected_hash, algorithm = read_signature(sig_path)\n",
    "            calculated_hash = calculate_hash(csv_path, algorithm=algorithm)\n",
    "            sig_filename = os.path.basename(sig_path)\n",
    "\n",
    "            if calculated_hash == expected_hash:\n",
    "                shutil.move(csv_path, os.path.join(input_dir, filename))\n",
    "                shutil.move(sig_path, os.path.join(archive_dir, sig_filename))\n",
    "                log_event(filename, \"OK\", f\"Integridad verificada ({algorithm}): {calculated_hash}\")\n",
    "            else:\n",
    "                shutil.move(csv_path, os.path.join(rejected_dir, filename))\n",
    "                shutil.move(sig_path, os.path.join(archive_dir, sig_filename))\n",
    "                log_event(filename, \"REJECTED\",\n",
    "                          f\"Hash incorrecto ({algorithm}). Esperado: {expected_hash} | Calculado: {calculated_hash}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log_event(filename, \"ERROR\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "move-files-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-18 16:48:36 | stock_snapshot_ES_WH_02_20260129.csv | REJECTED | Hash incorrecto (sha256). Esperado: 31b69a91ae0a42b48373b69279573b9d2612eb4dc9f7c142e1deb81f8e8d80ab | Calculado: 451aba4bcdcff581344f2bba735e703a94b15f63c7f5049deb3e36559afb2120\n",
      "2026-02-18 16:48:36 | stock_snapshot_ES_WH_06_20260127.csv | REJECTED | Hash incorrecto (md5). Esperado: 854e5d63a7eaf0f7f11e10cb39b653cb | Calculado: 3570c6d4dd3028300334fa474fd6e209\n",
      "2026-02-18 16:48:36 | stock_snapshot_FR_WH_02_20260129.csv | REJECTED | Hash incorrecto (sha256). Esperado: a7039cf19f7555f9f11b600adc18b7b2c215b7ecd89122db08df90dea845abcd | Calculado: 59271123818f76ead8b8d1ceec36f5825027003e9ac500d3654432dcc05a36f2\n",
      "2026-02-18 16:48:36 | stock_snapshot_ES_WH_04_20260128.csv | OK | Integridad verificada (sha256): 3b8d3ed4e7c293e25ad0b8ab7fe1738c34f603530eec800b3624d869f62fa25f\n",
      "2026-02-18 16:48:36 | stock_snapshot_ES_WH_01_20260129.csv | OK | Integridad verificada (sha256): 6a4f07384d1e2c459e9204b88462996fb3bc4d18be60d127d1b274c93cc44bef\n",
      "2026-02-18 16:48:36 | stock_snapshot_FR_WH_01_20260129.csv | REJECTED | No se encontr√≥ fichero de firma asociado\n",
      "2026-02-18 16:48:36 | stock_snapshot_PT_WH_01_20260129.csv | OK | Integridad verificada (md5): c1ab952467e26b47dbc18f829b33412d\n",
      "2026-02-18 16:48:36 | stock_snapshot_ES_WH_03_20260128.csv | OK | Integridad verificada (sha256): 6496c67f773b2e7b416e5b5bc1eb645197f0abe3555203469e57938255af8540\n",
      "2026-02-18 16:48:36 | stock_snapshot_PT_WH_02_20260129.csv | OK | Integridad verificada (md5): 35c04b7f0a736185b725f1d470bc5f1b\n"
     ]
    }
   ],
   "source": [
    "process_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3971932e",
   "metadata": {},
   "source": [
    "### Tarea 3 ‚Äì Conteo de ficheros correctos e incorrectos\n",
    "\n",
    "Mostrar el n√∫mero total de ficheros procesados correctamente y los rechazados, para validar la actividad.  \n",
    "\n",
    "Puede usarse cat y grep sobre el fichero de log generado por el proceso para obtener esta parte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "count-files-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ficheros aceptados (OK):       5\n",
      "‚ùå Ficheros rechazados (REJECTED): 4\n",
      "üì¶ Total procesados:              9\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"grep\", \"-c\", \"| OK |\", LOG_FILE],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "ok_count = int(result.stdout.strip()) if result.returncode == 0 else 0\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"grep\", \"-c\", \"| REJECTED |\", LOG_FILE],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "rejected_count = int(result.stdout.strip()) if result.returncode == 0 else 0\n",
    "\n",
    "print(f\"‚úÖ Ficheros aceptados (OK):       {ok_count}\")\n",
    "print(f\"‚ùå Ficheros rechazados (REJECTED): {rejected_count}\")\n",
    "print(f\"üì¶ Total procesados:              {ok_count + rejected_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a605caf6-1d8c-4ed7-851a-4bbd78322aeb",
   "metadata": {},
   "source": [
    "reset_data_pipeline()\r\n",
    "prepare_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8411f94-20d5-48f8-be48-74f8a8c9535d",
   "metadata": {},
   "source": [
    "## Declaraci√≥n de autor√≠a y uso de Inteligencia Artificial Generativa (IA)\n",
    "\n",
    "Durante la realizaci√≥n de esta actividad, declaro que (sustituye el cuadro por una X donde proceda):\n",
    "\n",
    "‚òê **No he utilizado herramientas de IA** durante la realizaci√≥n de esta actividad.  \n",
    "\n",
    "‚òê **He utilizado herramientas de IA como apoyo al aprendizaje**, para la consulta puntual de conceptos, aclaraci√≥n de dudas o comprensi√≥n de la documentaci√≥n, elaborando de forma aut√≥noma el contenido entregado.  \n",
    "\n",
    "‚òê **Parte del contenido de la actividad ha sido generado con herramientas de IA**, siendo dicho contenido revisado, comprendido y adaptado cr√≠ticamente antes de su entrega final.  \n",
    "\n",
    "‚òê **La mayor parte del contenido de la actividad ha sido generado con herramientas de IA**, existiendo una aportaci√≥n propia limitada.  \n",
    "\n",
    "La selecci√≥n de esta opci√≥n implica asumir la responsabilidad sobre la autor√≠a, comprensi√≥n y calidad del trabajo presentado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0e2b6-7824-4194-b184-849bba3a8816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
